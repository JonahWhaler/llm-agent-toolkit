import os
import io
import logging
import openai

from ..._core import A2T_Core
from ..._audio import AudioHelper
from ..._util import (
    CreatorRole,
    TranscriptionConfig,
    MessageBlock,
)
from .base import OpenAICore

logger = logging.getLogger(__name__)


class A2T_OAI_Core(A2T_Core, OpenAICore):
    """
    `A2T_OAI_Core` is a concrete implementation of the `A2T_Core` abstract class.
    `A2T_OAI_Core` is also a child class of `OpenAICore`.

    It facilitates synchronous and asynchronous communication with OpenAI's API to transcribe audio files.

    Methods:
    - run(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> list[MessageBlock | dict]:
        Synchronously run the LLM model to transcribe audio file.
    - run_async(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> list[MessageBlock | dict]:
        Asynchronously run the LLM model to transcribe audio file.

    Notes:
    - Only accept audio file in OGG and MP3 format!!!
    - Large audio files will be split into multiple chunks, overlapping is supported.
    - Generated file are stored with md format.
    - Tools are not supported in current version.
    - Context is not supported in current version.
    """

    def __init__(
        self,
        system_prompt: str,
        config: TranscriptionConfig,
    ):
        assert isinstance(config, TranscriptionConfig)
        A2T_Core.__init__(self, system_prompt, config)
        OpenAICore.__init__(self, config.name)
        self.__profile = self.build_profile(config.name)
        if not self.profile["audio_input"]:
            logger.warning("%s does not support audio input.", config.name)

    @property
    def profile(self) -> dict:
        """
        Profile is mostly for view purpose only,
        except the context_length which might be used to control the input to the LLM.
        """
        return self.__profile

    async def run_async(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> list[MessageBlock | dict]:
        """
        Asynchronously run the LLM model to transcribe audio file.

        Args:
            query (str):
                The query to be transcribed.
            context (list[MessageBlock | dict] | None):
                The context to be used for the query, not supported in current version.
            filepath (str):
                The path to the audio file to be transcribed.
            tmp_directory (str):
                The path to the temporary directory to store the audio chunks.

        Returns:
            list[MessageBlock | dict]: The list of messages generated by the LLM model.
        """
        filepath: str | None = kwargs.get("filepath", None)
        tmp_directory = kwargs.get("tmp_directory", None)
        if filepath is None or tmp_directory is None:
            raise ValueError("filepath and tmp_directory are required")
        ext = os.path.splitext(filepath)[-1]
        params = self.config.__dict__
        params["model"] = self.model_name
        for kw in ["name", "return_n", "max_iteration"]:
            del params[kw]
        try:
            output: list[MessageBlock] = []
            client = openai.AsyncOpenAI(api_key=os.environ["OPENAI_API_KEY"])
            chunks = AudioHelper.generate_chunks(
                input_path=filepath, tmp_directory=tmp_directory, output_format=ext[1:]
            )
            for idx, chunk_path in enumerate(chunks):
                with open(chunk_path, "rb") as f:
                    audio_data = f.read()
                    buffer = io.BytesIO(audio_data)
                    buffer.name = filepath
                    buffer.seek(0)

                params["file"] = buffer
                params["prompt"] = (
                    f"SYSTEM={self.system_prompt}\nQUERY={query}\nPage={idx+1}"
                )
                transcript = await client.audio.transcriptions.create(**params)
                # BEGIN DEBUG
                filename_wo_ext = os.path.basename(chunk_path).split(".")[0]
                export_path = f"{tmp_directory}/{filename_wo_ext}.md"
                with open(export_path, "w", encoding="utf-8") as writer:
                    writer.write(transcript.strip())
                # END DEBUG
                output.append(
                    MessageBlock(
                        role=CreatorRole.ASSISTANT.value,
                        content=f"Page={idx+1}\n{transcript.strip()}",
                    )
                )
            return [*output]
        except Exception as e:
            logger.error("Exception: %s", e)
            raise

    def run(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> list[MessageBlock | dict]:
        """
        Synchronously run the LLM model to transcribe audio file.

        Args:
            query (str):
                The query to be transcribed.
            context (list[MessageBlock | dict] | None):
                The context to be used for the query, not supported in current version.
            filepath (str):
                The path to the audio file to be transcribed.
            tmp_directory (str):
                The path to the temporary directory to store the audio chunks.

        Returns:
            list[MessageBlock | dict]: The list of messages generated by the LLM model.
        """
        filepath: str | None = kwargs.get("filepath", None)
        tmp_directory = kwargs.get("tmp_directory", None)
        if filepath is None or tmp_directory is None:
            raise ValueError("filepath and tmp_directory are required")
        ext = os.path.splitext(filepath)[-1]
        params = self.config.__dict__
        params["model"] = self.model_name
        for kw in ["name", "return_n", "max_iteration"]:
            del params[kw]

        try:
            output: list[MessageBlock] = []
            client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"])
            chunks = AudioHelper.generate_chunks(
                input_path=filepath, tmp_directory=tmp_directory, output_format=ext[1:]
            )
            for idx, chunk_path in enumerate(chunks):
                with open(chunk_path, "rb") as f:
                    audio_data = f.read()
                    buffer = io.BytesIO(audio_data)
                    buffer.name = filepath
                    buffer.seek(0)

                params["file"] = buffer
                params["prompt"] = (
                    f"SYSTEM={self.system_prompt}\nQUERY={query}\nPage={idx+1}"
                )
                transcript = client.audio.transcriptions.create(**params)
                # BEGIN DEBUG
                filename_wo_ext = os.path.basename(chunk_path).split(".")[0]
                export_path = f"{tmp_directory}/{filename_wo_ext}.md"
                with open(export_path, "w", encoding="utf-8") as writer:
                    writer.write(transcript.strip())
                # END DEBUG
                output.append(
                    MessageBlock(
                        role=CreatorRole.ASSISTANT.value,
                        content=f"Page={idx+1}\n{transcript.strip()}",
                    )
                )
            return [*output]
        except Exception as e:
            logger.error("Exception: %s", e)
            raise
