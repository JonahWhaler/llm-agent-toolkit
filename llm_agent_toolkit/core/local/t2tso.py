import json
import logging
from typing import Any
import ollama
from pydantic import BaseModel

from ..._core import Core
from ..._util import (
    CreatorRole,
    ChatCompletionConfig,
    MessageBlock,
)
from .base import OllamaCore

logger = logging.getLogger(__name__)


class T2TSO_OLM_Core(Core, OllamaCore):
    """
    `T2TSO_OLM_Core` is a concrete implementation of abstract base classes `Core`.
    `T2TSO_OLM_Core` is also a child class of `OllamaCore`.

    It facilitates synchronous and asynchronous communication with Ollama's API.

    Methods:
    - run(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> list[MessageBlock | dict]:
        Synchronously run the LLM model with the given query and context.
    - run_async(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> list[MessageBlock | dict]:
        Asynchronously run the LLM model with the given query and context.

    Notes:
    - The caller is responsible for memory management, output parsing and error handling.
    - If model is not available locally, an attempt to pull it from Ollama's server will be made.
    - `context_length` is configurable.
    - `max_output_tokens` is configurable.
    """

    def __init__(
        self,
        connection_string: str,
        system_prompt: str,
        config: ChatCompletionConfig,
    ):
        Core.__init__(self, system_prompt, config)
        OllamaCore.__init__(self, connection_string, config.name)
        self.profile = self.build_profile(model_name=config.name)

    async def run_async(
        self,
        query: str,
        context: list[dict | MessageBlock] | None,
        **kwargs,
    ) -> list[MessageBlock | dict]:
        """
        Asynchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            list[MessageBlock|dict]: The list of messages generated by the LLM model.
            Only 1 element in the list, it's content can be decoded by `json.loads()`
        """
        msgs: list[dict[str, Any] | MessageBlock] = [
            {"role": CreatorRole.SYSTEM.value, "content": self.system_prompt}
        ]

        if context:
            msgs.extend(context)
        msgs.append({"role": CreatorRole.USER.value, "content": query})

        response_format: BaseModel = kwargs.get("format", None)
        if response_format and type(response_format).__name__ != "ModelMetaclass":
            raise TypeError(
                f"Expect format to be type 'BaseModel', got '{type(response_format).__name__}'."
            )

        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        prompt_token_count = self.calculate_token_count(msgs, None)
        max_output_tokens = min(
            MAX_OUTPUT_TOKENS,
            self.context_length - prompt_token_count,
        )

        accumulated_token_count = 0  # Accumulated token count across iterations
        try:
            client = ollama.AsyncClient(host=self.CONN_STRING)
            if max_output_tokens > 0:
                response = await client.chat(
                    model=self.model_name,
                    messages=msgs,
                    tools=None,
                    stream=False,
                    format=response_format.model_json_schema(),
                    options={
                        "temperature": self.config.temperature,
                        "num_predict": max_output_tokens,
                    },
                )

                llm_generated_content: str = response["message"]["content"]

                if llm_generated_content:
                    if response_format:
                        try:
                            _ = json.loads(llm_generated_content)
                            content = llm_generated_content
                        except json.JSONDecodeError as decode_error:
                            e = {"error": str(decode_error)}
                            content = json.dumps(e)
                    else:
                        content = llm_generated_content

                    accumulated_token_count += (
                        response["eval_count"] + response["prompt_eval_count"]
                    )

                    return [{"role": CreatorRole.ASSISTANT.value, "content": content}]
            return []
        except Exception as e:
            logger.error("Exception: %s", e)
            raise

    def run(
        self, query: str, context: list[MessageBlock | dict[str, Any]] | None, **kwargs
    ) -> list[MessageBlock | dict[str, Any]]:
        """
        Synchronously generate text based on the given query and context.

        Args:
            query (str): The query to generate text for.
            context (list): A list of context messages or dictionaries.
            **kwargs: Additional keyword arguments.

        Returns:
            list[MessageBlock|dict]: The list of messages generated by the LLM model.
            Only 1 element in the list, it's content can be decoded by `json.loads()`
        """
        msgs: list[dict[str, Any] | MessageBlock] = [
            {"role": CreatorRole.SYSTEM.value, "content": self.system_prompt}
        ]

        if context:
            msgs.extend(context)
        msgs.append({"role": CreatorRole.USER.value, "content": query})

        response_format: BaseModel = kwargs.get("format", None)
        if response_format and type(response_format).__name__ != "ModelMetaclass":
            raise TypeError(
                f"Expect format to be type 'BaseModel', got '{type(response_format).__name__}'."
            )

        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        prompt_token_count = self.calculate_token_count(msgs, None)
        max_output_tokens = min(
            MAX_OUTPUT_TOKENS,
            self.context_length - prompt_token_count,
        )

        accumulated_token_count = 0  # Accumulated token count across iterations

        try:
            client = ollama.Client(host=self.CONN_STRING)
            if max_output_tokens <= 0:
                logger.info("Context-length: %d", self.context_length)
                logger.info("Prompt token count: %d", prompt_token_count)
                raise ValueError("max_output_tokens <= 0")

            response = client.chat(
                model=self.model_name,
                messages=msgs,
                tools=None,
                stream=False,
                format=response_format.model_json_schema(),
                options={
                    "temperature": self.config.temperature,
                    "num_predict": max_output_tokens,
                },
            )

            llm_generated_content: str = response["message"]["content"]

            if llm_generated_content:
                if response_format:
                    try:
                        _ = json.loads(llm_generated_content)
                        content = llm_generated_content
                    except json.JSONDecodeError as decode_error:
                        e = {"error": str(decode_error)}
                        content = json.dumps(e)
                else:
                    content = llm_generated_content

                accumulated_token_count += (
                    response["eval_count"] + response["prompt_eval_count"]
                )

                return [{"role": CreatorRole.ASSISTANT.value, "content": content}]
            return []
        except Exception as e:
            logger.error("Exception: %s", e)
            raise
