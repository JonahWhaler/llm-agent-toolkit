import os
import logging
import ollama
from ..._core import I2T_Core, TextGenerator
from ..._util import (
    CreatorRole,
    ChatCompletionConfig,
    MessageBlock,
)
from .base import OllamaCore

logger = logging.getLogger(__name__)


class I2T_OLM_Core(I2T_Core, OllamaCore, TextGenerator):  # , ToolSupport
    """
    `I2T_OLM_Core` is a concrete implementation of the `I2T_Core` abstract base class.
    `I2T_OLM_Core` is also a child class of:
    * `OllamaCore`.
    * `TextGenerator`

    It facilitates synchronous and asynchronous communication with ollama's API to interpret images.

    **Methods:**
    - run(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> list[MessageBlock | dict]:
        Synchronously run the LLM model to interpret images.
    - run_async(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> list[MessageBlock | dict]:
        Asynchronously run the LLM model to interpret images.
    - get_image_url(filepath: str) -> str:
        Returns the URL of the image from the specified file path.

    **Notes:**
    - Supported image format: .png, .jpeg, .jpg, .gif, .webp
    - Tools are not supported. At the point of implementing, Ollama have not release models that support both vision and tool.
    - The caller is responsible for memory management, output parsing and error handling.
    - The caller is responsible for choosing models that support `Vision`.
    - Keep the while loop of run and run_async, hoping that tools will be supported someday.
    """

    SUPPORTED_IMAGE_FORMATS = (".png", ".jpeg", ".jpg", ".gif", ".webp")

    def __init__(
        self,
        connection_string: str,
        system_prompt: str,
        config: ChatCompletionConfig,
    ):
        I2T_Core.__init__(self, system_prompt, config)
        OllamaCore.__init__(self, connection_string, config.name)
        self.__profile = self.build_profile(model_name=config.name)
        if self.profile["image_input"] is False:
            logger.warning("Vision might not work on this %s", self.model_name)

    @property
    def context_length(self) -> int:
        return self.profile["context_length"]

    @context_length.setter
    def context_length(self, value):
        """
        Set the context length.
        It shall be the user's responsiblity to ensure this is a model supported context length.

        Args:
            context_length (int): Context length to be set.

        Returns:
            None

        Raises:
            TypeError: If context_length is not type int.
            ValueError: If context_length is <= 0.
        """
        if not isinstance(value, int):
            raise TypeError(
                f"Expect context_length to be type 'int', got '{type(value).__name__}'."
            )
        if value <= 0:
            raise ValueError("Expect context_length > 0.")

        self.__profile["context_length"] = value

    @property
    def profile(self) -> dict:
        """
        Profile is mostly for view purpose only,
        except the context_length which might be used to control the input to the LLM.
        """
        return self.__profile

    @staticmethod
    def get_image_url(filepath: str) -> str:
        return ""

    def run(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> list[MessageBlock | dict]:
        """
        Synchronously run the LLM model to interpret images.

        Args:
            query (str): The query to be interpreted.
            context (list[MessageBlock | dict] | None): The context to be used for the query.
            filepath (str): The path to the image file to be interpreted.

        Returns:
            list[MessageBlock | dict]: The list of messages generated by the LLM model.
        """
        msgs: list[MessageBlock | dict] = [
            MessageBlock(role=CreatorRole.SYSTEM.value, content=self.system_prompt)
        ]

        if context is not None:
            msgs.extend(context)

        filepath: str | None = kwargs.get("filepath", None)

        if filepath:
            # Validation Step
            ext = os.path.splitext(filepath)[-1]
            ext = ext.lower()
            if ext not in I2T_OLM_Core.SUPPORTED_IMAGE_FORMATS:
                raise ValueError(f"Unsupported image type: {ext}")
            msgs.append(
                {"role": CreatorRole.USER.value, "content": query, "images": [filepath]}
            )

        number_of_primers = len(msgs)
        if isinstance(self.config, ChatCompletionConfig):
            temperature = self.config.temperature
            max_tokens = self.config.max_tokens
        else:
            temperature = 0.7
            max_tokens = 4096

        max_tokens = min(max_tokens, self.context_length)

        iteration = 0
        token_count = 0
        solved = False

        try:
            client = ollama.Client(host=self.CONN_STRING)
            while (
                not solved
                and iteration < self.config.max_iteration
                and token_count < max_tokens
            ):
                # print(f"\n\nIteration: {iteration}")
                response = client.chat(
                    model=self.model_name,
                    messages=msgs,
                    tools=None,
                    stream=False,
                    options={"temperature": temperature, "num_predict": max_tokens},
                )
                token_count += response["eval_count"] + response["prompt_eval_count"]

                llm_generated_content = response["message"]["content"]
                msgs.append(
                    MessageBlock(
                        role=CreatorRole.ASSISTANT.value,
                        content=llm_generated_content,
                    )
                )
                solved = True
                iteration += 1

            if not solved:
                if iteration == self.config.max_iteration:
                    logger.warning(
                        "Maximum iteration reached. %d/%d",
                        iteration,
                        self.config.max_iteration,
                    )
                elif token_count >= max_tokens:
                    logger.warning(
                        "Maximum token count reached. %d/%d", token_count, max_tokens
                    )
            return msgs[number_of_primers:]  # Return only the generated messages
        except Exception as e:
            logger.error("Error: %s", e)
            raise

    async def run_async(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> list[MessageBlock | dict]:
        """
        Synchronously run the LLM model to interpret images.

        Args:
            query (str): The query to be interpreted.
            context (list[MessageBlock | dict] | None): The context to be used for the query.
            filepath (str): The path to the image file to be interpreted.

        Returns:
            list[MessageBlock | dict]: The list of messages generated by the LLM model.
        """
        msgs: list[MessageBlock | dict] = [
            MessageBlock(role=CreatorRole.SYSTEM.value, content=self.system_prompt)
        ]

        if context is not None:
            msgs.extend(context)

        filepath: str | None = kwargs.get("filepath", None)
        if filepath:
            # Validation Step
            ext = os.path.splitext(filepath)[-1]
            ext = ext.lower()
            if ext not in I2T_OLM_Core.SUPPORTED_IMAGE_FORMATS:
                raise ValueError(f"Unsupported image type: {ext}")
            msgs.append(
                {"role": CreatorRole.USER.value, "content": query, "images": [filepath]}
            )

        number_of_primers = len(msgs)
        if isinstance(self.config, ChatCompletionConfig):
            temperature = self.config.temperature
            max_tokens = self.config.max_tokens
        else:
            temperature = 0.7
            max_tokens = 4096

        max_tokens = min(max_tokens, self.context_length)

        iteration = 0
        token_count = 0
        solved = False

        try:
            client = ollama.AsyncClient(host=self.CONN_STRING)
            while (
                not solved
                and iteration < self.config.max_iteration
                and token_count < max_tokens
            ):
                # print(f"\n\nIteration: {iteration}")
                response = await client.chat(
                    model=self.model_name,
                    messages=msgs,
                    tools=None,
                    stream=False,
                    options={"temperature": temperature, "num_predict": max_tokens},
                )
                token_count += response["eval_count"] + response["prompt_eval_count"]

                llm_generated_content = response["message"]["content"]
                msgs.append(
                    MessageBlock(
                        role=CreatorRole.ASSISTANT.value,
                        content=llm_generated_content,
                    )
                )
                solved = True
                iteration += 1

            if not solved:
                if iteration == self.config.max_iteration:
                    logger.warning(
                        "Maximum iteration reached. %d/%d",
                        iteration,
                        self.config.max_iteration,
                    )
                elif token_count >= max_tokens:
                    logger.warning(
                        "Maximum token count reached. %d/%d", token_count, max_tokens
                    )
            return msgs[number_of_primers:]  # Return only the generated messages
        except Exception as e:
            logger.error("Error: %s", e)
            raise
